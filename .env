# ===== LLM Provider Configuration =====
# Options: "ollama" (local GPU) or "gemini" (cloud API)
LLM_PROVIDER=ollama

# Ollama Settings (GPU Local - Free)
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_URL=http://localhost:11434

# Gemini Settings (Cloud API - Fallback option)
# GOOGLE_API_KEY=
GEMINI_MODEL=gemini-2.5-flash

# ===== RAG Configuration =====
EMBEDDING_MODEL=BAAI/bge-m3
CHROMA_PATH=./data/chroma_db

# Chunking parameters
CHUNK_SIZE=400
CHUNK_OVERLAP=50

# Retrieval parameters
TOP_K=5

# ===== Performance Optimization =====
# Embedding optimization
EMBEDDING_BATCH_SIZE=32
USE_FP16=true
