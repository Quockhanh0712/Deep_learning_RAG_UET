"""
Vietnamese Legal RAG Chatbot - Chainlit Interface

Chatbot há»i Ä‘Ã¡p phÃ¡p luáº­t Viá»‡t Nam vá»›i:
- Graph Database: LÆ°u trá»¯ cáº¥u trÃºc phÃ¢n cáº¥p
- Hybrid Search: Vector + Graph context enrichment
- Vietnamese Legal Embedding (768D)
- Local LLM (Qwen3-4B hoáº·c Ollama)
"""

import os
import sys
from pathlib import Path
from typing import List, Dict
import asyncio

# Add project root to path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

import chainlit as cl
from dotenv import load_dotenv

# Load environment variables FIRST
load_dotenv()

# Configure
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
TOP_K = int(os.getenv("TOP_K", "15"))  # More context for better answers

# Global caching for models - PRELOAD to avoid reload
_hybrid_search = None
_graph_db = None
_llm_preloaded = False


def get_hybrid_search():
    """Lazy load HybridLegalSearch"""
    global _hybrid_search
    if _hybrid_search is None:
        from src.hybrid_legal_search import HybridLegalSearch
        _hybrid_search = HybridLegalSearch()
    return _hybrid_search


def get_graph_db():
    """Lazy load GraphDB"""
    global _graph_db
    if _graph_db is None:
        from src.graph_db import LegalGraphDB
        _graph_db = LegalGraphDB()
    return _graph_db


def preload_models():
    """Preload HuggingFace model ONLY if using transformers provider"""
    global _llm_preloaded
    if not _llm_preloaded:
        provider = os.getenv("LLM_PROVIDER", "ollama")
        if provider == "transformers":
            try:
                from src.llm_client import _load_hf_model
                print("[PRELOAD] Loading HuggingFace model at startup...")
                _load_hf_model()
                _llm_preloaded = True
                print("[PRELOAD] Model loaded successfully!")
            except Exception as e:
                print(f"[PRELOAD] Warning: Could not preload model: {e}")
        else:
            print(f"[PRELOAD] Using provider: {provider} - No preload needed")
            _llm_preloaded = True


def build_legal_prompt(question: str, context: str) -> str:
    """Build prompt for legal Q&A - Optimized for Ollama"""
    # Detailed legal prompt with Vietnamese focus
    return f"""Báº¡n lÃ  chuyÃªn gia phÃ¡p luáº­t Viá»‡t Nam vá»›i nhiá»u nÄƒm kinh nghiá»‡m. LUÃ”N tráº£ lá»i Báº°NG TIáº¾NG VIá»†T.

## VÄ‚N Báº¢N PHÃP LUáº¬T THAM KHáº¢O:
{context}

## CÃ‚U Há»I:
{question}

## YÃŠU Cáº¦U TRáº¢ Lá»œI:
1. Tráº£ lá»i CHI TIáº¾T, Cá»¤ THá»‚ dá»±a trÃªn vÄƒn báº£n phÃ¡p luáº­t trÃªn
2. TrÃ­ch dáº«n RÃ• RÃ€NG: tÃªn luáº­t, sá»‘/nÄƒm, Ä‘iá»u, khoáº£n, Ä‘iá»ƒm
3. Giáº£i thÃ­ch THá»°C Táº¾, dá»… hiá»ƒu cho ngÆ°á»i dÃ¢n
4. Báº°NG TIáº¾NG VIá»†T, khÃ´ng dÃ¹ng tiáº¿ng Anh

## TRáº¢ Lá»œI:

VÄ‚N Báº¢N PHÃP LUáº¬T:
{context}

CÃ‚U Há»I: {question}

TRáº¢ Lá»œI (Báº°NG TIáº¾NG VIá»†T):

TRáº¢ Lá»œI:"""


def sync_legal_query(question: str, strategy: str = 'graph_enhanced', top_k: int = 5) -> Dict:
    """Process legal query with hybrid search"""
    hybrid = get_hybrid_search()
    
    # Search
    results = hybrid.search(query=question, strategy=strategy, k=top_k)
    
    if not results:
        return {
            "answer": "âŒ KhÃ´ng tÃ¬m tháº¥y vÄƒn báº£n phÃ¡p luáº­t liÃªn quan. Vui lÃ²ng thá»­ vá»›i tá»« khÃ³a khÃ¡c hoáº·c diá»…n Ä‘áº¡t cÃ¢u há»i rÃµ rÃ ng hÆ¡n.",
            "sources": [],
            "context": ""
        }
    
    # Build context
    context = hybrid.build_rag_context(results)
    
    # Generate answer
    from src.llm_client import generate_answer
    prompt = build_legal_prompt(question, context)
    answer = generate_answer(prompt)
    
    # Build sources list - show more content
    sources = []
    for r in results:
        sources.append({
            "law_id": r.law_id,
            "article_title": r.article_title,
            "clause_id": r.clause_id,
            "point_id": r.point_id,
            "content": r.content[:500] + "..." if len(r.content) > 500 else r.content,
            "score": r.score
        })
    
    return {
        "answer": answer,
        "sources": sources,
        "context": context
    }


def format_sources_markdown(sources: List[Dict]) -> str:
    """Format sources as markdown - SIMPLIFIED"""
    if not sources:
        return ""
    
    md = "\n\n---\n## ğŸ“š Nguá»“n tham kháº£o\n\n"
    
    for i, s in enumerate(sources, 1):
        # Simple header
        header = f"**{s['law_id']}**"
        if s.get('article_title'):
            header += f" - {s['article_title']}"
        
        md += f"{i}. {header} (Score: {s['score']:.2f})\n"
        
        # Show content in collapsed format
        content_preview = s['content'][:150] + "..." if len(s['content']) > 150 else s['content']
        md += f"   > {content_preview}\n\n"
    
    return md


@cl.on_chat_start
async def on_chat_start():
    """Initialize the chat session."""
    # Preload models ONCE at startup
    preload_models()
    
    # Get statistics
    try:
        graph = get_graph_db()
        stats = graph.stats()
        
        stats_str = f"""
| Loáº¡i | Sá»‘ lÆ°á»£ng |
|------|----------|
| ğŸ“š VÄƒn báº£n | {stats['node_types'].get('law', 0):,} |
| ğŸ“„ Äiá»u | {stats['node_types'].get('article', 0):,} |
| ğŸ“ Khoáº£n | {stats['node_types'].get('clause', 0):,} |
| ğŸ”¹ Äiá»ƒm | {stats['node_types'].get('point', 0):,} |
| **Tá»•ng** | **{stats['total_nodes']:,}** |
"""
    except Exception as e:
        stats_str = f"_KhÃ´ng thá»ƒ load thá»‘ng kÃª: {e}_"
    
    # Welcome message
    welcome_msg = f"""# âš–ï¸ Trá»£ lÃ½ PhÃ¡p luáº­t Viá»‡t Nam

Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam.

## ğŸ“Š CÆ¡ sá»Ÿ dá»¯ liá»‡u phÃ¡p luáº­t
{stats_str}

## ğŸš€ TÃ­nh nÄƒng
- ğŸ” **Hybrid Search**: Káº¿t há»£p tÃ¬m kiáº¿m vector + graph
- ğŸ“š **Graph Database**: Cáº¥u trÃºc phÃ¢n cáº¥p Luáº­t â†’ Äiá»u â†’ Khoáº£n â†’ Äiá»ƒm
- ğŸ‡»ğŸ‡³ **Vietnamese NLP**: Tokenization tiáº¿ng Viá»‡t chuyÃªn biá»‡t
- ğŸ“– **TrÃ­ch nguá»“n**: LuÃ´n kÃ¨m nguá»“n tham kháº£o

## ğŸ’¡ CÃ¢u há»i máº«u
- Äiá»u kiá»‡n káº¿t hÃ´n theo phÃ¡p luáº­t Viá»‡t Nam?
- Thá»§ tá»¥c Ä‘Äƒng kÃ½ doanh nghiá»‡p?
- Quy Ä‘á»‹nh vá» há»£p Ä‘á»“ng lao Ä‘á»™ng?
- Äá»™ tuá»•i chá»‹u trÃ¡ch nhiá»‡m hÃ¬nh sá»±?

## ğŸ“± Commands
- `/stats` - Xem thá»‘ng kÃª cÆ¡ sá»Ÿ dá»¯ liá»‡u
- `/search <tá»« khÃ³a>` - TÃ¬m kiáº¿m nhanh
- `/law <mÃ£ luáº­t>` - Xem chi tiáº¿t vÄƒn báº£n
- `/help` - Trá»£ giÃºp

---
**HÃ£y Ä‘áº·t cÃ¢u há»i phÃ¡p luáº­t cá»§a báº¡n! âš–ï¸**
"""
    
    await cl.Message(content=welcome_msg).send()
    
    # Store settings in session
    cl.user_session.set("search_strategy", "graph_enhanced")
    cl.user_session.set("top_k", TOP_K)


@cl.on_message
async def on_message(message: cl.Message):
    """Handle incoming messages."""
    user_input = message.content.strip()
    
    # Handle commands
    if user_input.lower() == "/help":
        help_msg = """## â“ Trá»£ giÃºp

### Commands
| Command | MÃ´ táº£ |
|---------|-------|
| `/stats` | Xem thá»‘ng kÃª cÆ¡ sá»Ÿ dá»¯ liá»‡u |
| `/search <tá»« khÃ³a>` | TÃ¬m kiáº¿m tá»« khÃ³a trong vÄƒn báº£n |
| `/law <mÃ£ luáº­t>` | Xem chi tiáº¿t má»™t vÄƒn báº£n phÃ¡p luáº­t |
| `/mode` | Xem/Ä‘á»•i cháº¿ Ä‘á»™ tÃ¬m kiáº¿m |
| `/help` | Hiá»ƒn thá»‹ trá»£ giÃºp |

### CÃ¡ch há»i hiá»‡u quáº£
1. **Há»i cá»¥ thá»ƒ**: "Äiá»u kiá»‡n káº¿t hÃ´n" thay vÃ¬ "cho há»i vá» káº¿t hÃ´n"
2. **DÃ¹ng tá»« khÃ³a phÃ¡p lÃ½**: "quy Ä‘á»‹nh", "thá»§ tá»¥c", "Ä‘iá»u kiá»‡n", "xá»­ pháº¡t"
3. **NÃªu rÃµ lÄ©nh vá»±c**: "há»£p Ä‘á»“ng lao Ä‘á»™ng", "hÃ´n nhÃ¢n gia Ä‘Ã¬nh", "doanh nghiá»‡p"

### Chiáº¿n lÆ°á»£c tÃ¬m kiáº¿m
- `graph_enhanced` (máº·c Ä‘á»‹nh): Káº¿t há»£p vector + graph context
- `vector_only`: Chá»‰ tÃ¬m kiáº¿m ngá»¯ nghÄ©a
- `hierarchical`: TÃ¬m trong graph + má»Ÿ rá»™ng

Äá»•i báº±ng lá»‡nh `/mode <strategy>`
"""
        await cl.Message(content=help_msg).send()
        return
    
    if user_input.lower() == "/stats":
        try:
            graph = get_graph_db()
            stats = graph.stats()
            
            msg = f"""## ğŸ“Š Thá»‘ng kÃª CÆ¡ sá»Ÿ dá»¯ liá»‡u

| Loáº¡i | Sá»‘ lÆ°á»£ng |
|------|----------|
| ğŸ“š VÄƒn báº£n luáº­t | {stats['node_types'].get('law', 0):,} |
| ğŸ“„ Äiá»u | {stats['node_types'].get('article', 0):,} |
| ğŸ“ Khoáº£n | {stats['node_types'].get('clause', 0):,} |
| ğŸ”¹ Äiá»ƒm | {stats['node_types'].get('point', 0):,} |

**Tá»•ng nodes:** {stats['total_nodes']:,}
**Tá»•ng edges:** {stats['total_edges']:,}

---
_Graph Database: NetworkX vá»›i pickle persistence_
"""
            await cl.Message(content=msg).send()
        except Exception as e:
            await cl.Message(content=f"âŒ Lá»—i: {e}").send()
        return
    
    if user_input.lower().startswith("/search "):
        keyword = user_input[8:].strip()
        if not keyword:
            await cl.Message(content="âš ï¸ Vui lÃ²ng nháº­p tá»« khÃ³a: `/search <tá»« khÃ³a>`").send()
            return
        
        try:
            graph = get_graph_db()
            results = graph.search_by_content(keyword, limit=10)
            
            if results:
                msg = f"## ğŸ” Káº¿t quáº£ tÃ¬m kiáº¿m: \"{keyword}\"\n\n"
                for i, (node_id, data) in enumerate(results[:5], 1):
                    content = data.get('content', '')[:150]
                    msg += f"### {i}. {node_id[:50]}\n"
                    msg += f"> {content}...\n\n"
                
                msg += f"\n_TÃ¬m tháº¥y {len(results)} káº¿t quáº£_"
                await cl.Message(content=msg).send()
            else:
                await cl.Message(content=f"ğŸ“­ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ cho \"{keyword}\"").send()
        except Exception as e:
            await cl.Message(content=f"âŒ Lá»—i tÃ¬m kiáº¿m: {e}").send()
        return
    
    if user_input.lower().startswith("/law "):
        law_id = user_input[5:].strip()
        if not law_id:
            await cl.Message(content="âš ï¸ Vui lÃ²ng nháº­p mÃ£ vÄƒn báº£n: `/law <mÃ£>`").send()
            return
        
        try:
            graph = get_graph_db()
            results = graph.search_by_law(law_id)
            
            if results:
                msg = f"## ğŸ“œ VÄƒn báº£n: {law_id}\n\n"
                msg += f"**Sá»‘ Ä‘iá»u khoáº£n:** {len(results)}\n\n"
                
                # Group by article
                articles = {}
                for node_id, data in results[:20]:
                    art_id = data.get('article_id', 0)
                    if art_id not in articles:
                        articles[art_id] = data.get('article_title', f'Äiá»u {art_id}')
                
                for art_id, title in sorted(articles.items()):
                    msg += f"- **{title}**\n"
                
                if len(results) > 20:
                    msg += f"\n_...vÃ  {len(results) - 20} má»¥c khÃ¡c_"
                
                await cl.Message(content=msg).send()
            else:
                await cl.Message(content=f"ğŸ“­ KhÃ´ng tÃ¬m tháº¥y vÄƒn báº£n: {law_id}").send()
        except Exception as e:
            await cl.Message(content=f"âŒ Lá»—i: {e}").send()
        return
    
    if user_input.lower() == "/mode":
        strategy = cl.user_session.get("search_strategy", "graph_enhanced")
        msg = f"""## âš™ï¸ Cháº¿ Ä‘á»™ tÃ¬m kiáº¿m

**Hiá»‡n táº¡i:** `{strategy}`

**CÃ¡c cháº¿ Ä‘á»™:**
- `graph_enhanced` - Vector + Graph context (khuyáº¿n nghá»‹)
- `vector_only` - Chá»‰ tÃ¬m kiáº¿m vector
- `hierarchical` - Graph traversal + má»Ÿ rá»™ng

**Äá»•i cháº¿ Ä‘á»™:** `/mode <strategy>`
"""
        await cl.Message(content=msg).send()
        return
    
    if user_input.lower().startswith("/mode "):
        new_mode = user_input[6:].strip().lower()
        valid_modes = ['graph_enhanced', 'vector_only', 'hierarchical']
        if new_mode in valid_modes:
            cl.user_session.set("search_strategy", new_mode)
            await cl.Message(content=f"âœ… ÄÃ£ Ä‘á»•i cháº¿ Ä‘á»™ tÃ¬m kiáº¿m sang: `{new_mode}`").send()
        else:
            await cl.Message(content=f"âš ï¸ Cháº¿ Ä‘á»™ khÃ´ng há»£p lá»‡. Chá»n: {', '.join(valid_modes)}").send()
        return
    
    # Skip empty messages
    if not user_input:
        return
    
    # Process legal question
    await process_legal_question(user_input)


async def process_legal_question(question: str):
    """Process a legal question and send response"""
    strategy = cl.user_session.get("search_strategy", "graph_enhanced")
    top_k = cl.user_session.get("top_k", TOP_K)
    
    # Show thinking message
    thinking_msg = await cl.Message(content="ğŸ”„ Äang tÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch...").send()
    
    try:
        # Run query in thread pool
        result = await asyncio.to_thread(
            sync_legal_query,
            question,
            strategy,
            top_k
        )
        
        # Build response
        response = result["answer"]
        
        # Add sources
        if result["sources"]:
            response += format_sources_markdown(result["sources"])
        
        # Update thinking message with result
        await thinking_msg.remove()
        
        # Send final response
        await cl.Message(content=response).send()
        
    except Exception as e:
        await thinking_msg.remove()
        await cl.Message(content=f"âŒ Lá»—i xá»­ lÃ½: {str(e)}").send()
        raise


# Entry point
if __name__ == "__main__":
    from chainlit.cli import run_chainlit
    run_chainlit(__file__)
