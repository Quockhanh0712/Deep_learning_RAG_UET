"""
Professional RAG Chatbot Interface using Chainlit
A modern, fast, and beautiful chatbot UI with Advanced RAG features:
- Hybrid Search (BM25 + Dense)
- Cross-Encoder Reranking
- Query Transformation
- Citation System
"""

import os
from pathlib import Path
from typing import List, Optional
import asyncio
from datetime import datetime

import chainlit as cl
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

from src.file_manager import load_file, delete_file
from src.vector_store import list_files, query_documents
from src.llm_client import generate_answer
from src.chat_history import get_history_manager

# Configuration
UPLOAD_DIR = Path("data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
TOP_K = int(os.getenv("TOP_K", "5"))

# Advanced RAG settings
USE_ADVANCED_RAG = os.getenv("USE_ADVANCED_RAG", "true").lower() == "true"
USE_HYBRID_SEARCH = os.getenv("USE_HYBRID_SEARCH", "true").lower() == "true"
USE_RERANKER = os.getenv("USE_RERANKER", "true").lower() == "true"
USE_CITATIONS = os.getenv("USE_CITATIONS", "true").lower() == "true"


def build_prompt(question: str, docs: List[str]) -> str:
    """Build the prompt for the LLM."""
    context = "\n\n".join(docs)
    prompt = (
        "Báº¡n lÃ  trá»£ lÃ½ AI thÃ´ng minh vÃ  thÃ¢n thiá»‡n. "
        "Tráº£ lá»i chi tiáº¿t, chÃ­nh xÃ¡c vÃ  dá»… hiá»ƒu dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p.\n"
        "Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p, hÃ£y thÃ´ng bÃ¡o ráº±ng báº¡n khÃ´ng cÃ³ Ä‘á»§ thÃ´ng tin.\n\n"
        f" Ngá»¯ cáº£nh:\n{context}\n\n"
        f"â“ CÃ¢u há»i: {question}\n\n"
        "ğŸ’¡ Tráº£ lá»i:"
    )
    return prompt


def sync_process_query_basic(question: str) -> dict:
    """Basic RAG query (legacy mode)."""
    res = query_documents(question, k=TOP_K)
    docs = res["documents"][0]
    metadatas = res["metadatas"][0]
    
    prompt = build_prompt(question, docs)
    answer = generate_answer(prompt)
    
    return {
        "answer": answer,
        "context": docs,
        "metadatas": metadatas,
        "citations_md": "",
    }


def sync_process_query_advanced(question: str) -> dict:
    """Advanced RAG query with hybrid search, reranking, and citations."""
    try:
        from src.retrieval import advanced_query
        
        result = advanced_query(
            question=question,
            use_hybrid=USE_HYBRID_SEARCH,
            use_rerank=USE_RERANKER,
            use_citations=USE_CITATIONS,
            top_k=TOP_K
        )
        
        # Extract context and metadata from sources
        docs = [s.get("content", "") for s in result.sources]
        metadatas = [s.get("metadata", {}) for s in result.sources]
        
        # Add retrieval info to answer if available
        answer = result.answer
        
        # Add query transformation info
        if result.query_info.get("rewritten") and result.query_info["rewritten"] != result.query_info["original"]:
            answer += f"\n\n_ğŸ”„ Query Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u: \"{result.query_info['rewritten']}\"_"
        
        return {
            "answer": answer,
            "context": docs,
            "metadatas": metadatas,
            "citations_md": result.citations_markdown,
            "retrieval_info": result.retrieval_info,
        }
        
    except ImportError as e:
        # Fallback to basic if advanced modules not available
        print(f"[CHATBOT] Advanced RAG not available, falling back to basic: {e}")
        return sync_process_query_basic(question)
    except Exception as e:
        print(f"[CHATBOT] Advanced RAG error, falling back to basic: {e}")
        return sync_process_query_basic(question)


def sync_process_query(question: str) -> dict:
    """Process a question and return the answer with context."""
    if USE_ADVANCED_RAG:
        return sync_process_query_advanced(question)
    else:
        return sync_process_query_basic(question)


async def process_uploaded_file(file_path: str, file_name: str) -> str:
    """Process an uploaded file and add to vector store."""
    # Save to uploads directory
    save_path = UPLOAD_DIR / file_name
    
    # Copy file
    with open(file_path, "rb") as src:
        content = src.read()
    with open(save_path, "wb") as dst:
        dst.write(content)
    
    # Check if already loaded
    existing_files = list_files()
    already_loaded = any(fid.startswith(file_name + "-") for fid in existing_files)
    
    if already_loaded:
        return f"â„¹ï¸ File `{file_name}` Ä‘Ã£ Ä‘Æ°á»£c náº¡p trÆ°á»›c Ä‘Ã³."
    
    # Load file into vector store
    file_id = await asyncio.to_thread(load_file, str(save_path))
    return f"âœ… ÄÃ£ náº¡p file thÃ nh cÃ´ng!\n- **File:** `{file_name}`\n- **ID:** `{file_id}`"


@cl.on_chat_start
async def on_chat_start():
    """Initialize the chat session."""
    # Generate session ID
    session_id = cl.user_session.get("id")
    
    # Initialize chat history
    history_mgr = get_history_manager()
    conv_id = history_mgr.get_or_create_conversation(session_id)
    
    cl.user_session.set("conversation_id", conv_id)
    cl.user_session.set("history", [])
    
    # Load existing messages from database
    messages = history_mgr.get_conversation_messages(conv_id)
    if messages:
        loaded_history = [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
        cl.user_session.set("history", loaded_history)
        
        # Notify user about loaded history
        await cl.Message(
            content=f"ğŸ“œ ÄÃ£ táº£i {len(messages)} tin nháº¯n tá»« lá»‹ch sá»­."
        ).send()
    
    # Get list of loaded files
    files = list_files()
    
    # Build feature status
    features = []
    if USE_ADVANCED_RAG:
        features.append("ğŸš€ Advanced RAG")
    if USE_HYBRID_SEARCH:
        features.append("ğŸ” Hybrid Search")
    if USE_RERANKER:
        features.append("ğŸ“Š Reranking")
    if USE_CITATIONS:
        features.append("ğŸ“š Citations")
    
    feature_str = " | ".join(features) if features else "Basic RAG"
    
    # Welcome message
    welcome_msg = f"""# ğŸ¤– RAG Chatbot Pro

Xin chÃ o! TÃ´i lÃ  trá»£ lÃ½ AI.

**âœ¨ TÃ­nh nÄƒng:** {feature_str}

## ğŸ“‹ TÃ i liá»‡u Ä‘Ã£ náº¡p:
"""
    
    if files:
        for f in files:
            welcome_msg += f"- ğŸ“„ `{f}`\n"
    else:
        welcome_msg += "_ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c náº¡p._\n"
    
    welcome_msg += """
---
**ğŸ’¡ HÆ°á»›ng dáº«n:**
- Äáº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u Ä‘Ã£ náº¡p
- ÄÃ­nh kÃ¨m file PDF/TXT cÃ¹ng vá»›i tin nháº¯n Ä‘á»ƒ upload

**ğŸ“± Quick Commands:**
- `/history` - ğŸ“œ Xem lá»‹ch sá»­ conversations
- `/search <query>` - ğŸ” TÃ¬m kiáº¿m trong lá»‹ch sá»­
- `/export` - ğŸ“¥ Export conversation hiá»‡n táº¡i
- `/stats` - ğŸ“Š Xem thá»‘ng kÃª há»‡ thá»‘ng
- `/files` - ğŸ“ Danh sÃ¡ch tÃ i liá»‡u
- `/clear` - ğŸ§¹ XÃ³a lá»‹ch sá»­ chat
- `/help` - â“ Hiá»ƒn thá»‹ trá»£ giÃºp

HÃ£y Ä‘áº·t cÃ¢u há»i cá»§a báº¡n! ğŸš€
"""
    
    await cl.Message(content=welcome_msg).send()

@cl.on_message
async def on_message(message: cl.Message):
    """Handle incoming messages."""
    user_input = message.content.strip()
    
    # Handle file attachments first
    if message.elements:
        for element in message.elements:
            if hasattr(element, 'path') and element.path:
                file_name = getattr(element, 'name', os.path.basename(element.path))
                # Check file extension
                if file_name.lower().endswith(('.pdf', '.txt')):
                    await cl.Message(content=f"â³ Äang xá»­ lÃ½ file `{file_name}`...").send()
                    try:
                        result = await process_uploaded_file(element.path, file_name)
                        await cl.Message(content=result).send()
                    except Exception as e:
                        await cl.Message(content=f"âŒ Lá»—i khi náº¡p file: {str(e)}").send()
                else:
                    await cl.Message(content=f"âš ï¸ Chá»‰ há»— trá»£ file PDF vÃ  TXT. File `{file_name}` bá»‹ bá» qua.").send()
        
        # If only file upload, no question
        if not user_input:
            return
    
    # Handle special commands
    if user_input.lower() == "/files":
        files = list_files()
        if files:
            file_list = "\n".join([f"ğŸ“„ `{f}`" for f in files])
            await cl.Message(content=f"## ğŸ“š TÃ i liá»‡u Ä‘Ã£ náº¡p:\n{file_list}").send()
        else:
            await cl.Message(content="ğŸ“­ ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c náº¡p.").send()
        return
    
    if user_input.lower() == "/clear":
        cl.user_session.set("history", [])
        await cl.Message(content="ğŸ§¹ ÄÃ£ xÃ³a lá»‹ch sá»­ chat!").send()
        return
    
    if user_input.lower() == "/help":
        help_msg = """## ğŸ“– Trá»£ giÃºp

**Commands:**
- `/files` - Xem danh sÃ¡ch tÃ i liá»‡u Ä‘Ã£ náº¡p
- `/delete <file_id>` - XÃ³a tÃ i liá»‡u
- `/mode` - Xem cháº¿ Ä‘á»™ RAG hiá»‡n táº¡i
- `/clear` - XÃ³a lá»‹ch sá»­ chat
- `/help` - Hiá»ƒn thá»‹ trá»£ giÃºp

**Upload file:**
- Click vÃ o icon ğŸ“ Ä‘á»ƒ Ä‘Ã­nh kÃ¨m file PDF/TXT

**Há»i Ä‘Ã¡p:**
- Nháº­p cÃ¢u há»i liÃªn quan Ä‘áº¿n tÃ i liá»‡u Ä‘Ã£ náº¡p

**Advanced Features:**
- ğŸ” Hybrid Search: Káº¿t há»£p BM25 + Dense search
- ğŸ“Š Reranking: Cross-encoder Ä‘á»ƒ xáº¿p háº¡ng láº¡i káº¿t quáº£
- ğŸ“š Citations: Hiá»ƒn thá»‹ nguá»“n tham kháº£o
"""
        await cl.Message(content=help_msg).send()
        return
    
    if user_input.lower() == "/mode":
        mode_msg = f"""## âš™ï¸ Cháº¿ Ä‘á»™ RAG

**Advanced RAG:** {'âœ… Báº­t' if USE_ADVANCED_RAG else 'âŒ Táº¯t'}
**Hybrid Search:** {'âœ… Báº­t' if USE_HYBRID_SEARCH else 'âŒ Táº¯t'}
**Reranking:** {'âœ… Báº­t' if USE_RERANKER else 'âŒ Táº¯t'}
**Citations:** {'âœ… Báº­t' if USE_CITATIONS else 'âŒ Táº¯t'}

_Äá»ƒ thay Ä‘á»•i, cáº­p nháº­t biáº¿n mÃ´i trÆ°á»ng vÃ  khá»Ÿi Ä‘á»™ng láº¡i._

**Biáº¿n mÃ´i trÆ°á»ng:**
- `USE_ADVANCED_RAG=true/false`
- `USE_HYBRID_SEARCH=true/false`
- `USE_RERANKER=true/false`
- `USE_CITATIONS=true/false`
"""
        await cl.Message(content=mode_msg).send()
        return
    
    if user_input.lower().startswith("/delete "):
        file_id = user_input[8:].strip()
        try:
            delete_file(file_id)
            await cl.Message(content=f"ğŸ—‘ï¸ ÄÃ£ xÃ³a file: `{file_id}`").send()
        except Exception as e:
            await cl.Message(content=f"âŒ Lá»—i khi xÃ³a file: {str(e)}").send()
        return
    
    # NEW: Chat history commands
    if user_input.lower() == "/history":
        """Show conversation history"""
        history_mgr = get_history_manager()
        conversations = history_mgr.list_conversations(limit=10)
        
        if conversations:
            msg = "# ğŸ“œ Lá»‹ch sá»­ Conversations\n\n"
            current_conv_id = cl.user_session.get("conversation_id")
            
            for i, conv in enumerate(conversations, 1):
                is_current = "ğŸŸ¢ **ÄANG Má»**" if conv.id == current_conv_id else ""
                msg += f"### {i}. {conv.title} {is_current}\n\n"
                msg += f"| | |\n|---|---|\n"
                msg += f"| **ID** | `{conv.id}` |\n"
                msg += f"| **Session** | `{conv.session_id[:12]}...` |\n"
                msg += f"| **Created** | {conv.created_at} |\n"
                msg += f"| **Updated** | {conv.updated_at} |\n\n"
                msg += "---\n\n"
            
            msg += f"\n_Hiá»ƒn thá»‹ {len(conversations)} conversations gáº§n nháº¥t_"
            await cl.Message(content=msg).send()
        else:
            await cl.Message(content="ğŸ“­ ChÆ°a cÃ³ lá»‹ch sá»­ conversation.").send()
        return
    
    if user_input.lower().startswith("/search "):
        """Search in chat history"""
        query = user_input[8:].strip()
        if not query:
            await cl.Message(content="âš ï¸ Vui lÃ²ng nháº­p tá»« khÃ³a tÃ¬m kiáº¿m: `/search <tá»« khÃ³a>`").send()
            return
        
        history_mgr = get_history_manager()
        results = history_mgr.search_messages(query, limit=5)
        
        if results:
            msg = f"# ğŸ” Káº¿t quáº£ TÃ¬m kiáº¿m\n\n"
            msg += f"**Tá»« khÃ³a:** `{query}`  \n"
            msg += f"**Sá»‘ káº¿t quáº£:** {len(results)}\n\n"
            msg += "---\n\n"
            
            for i, r in enumerate(results, 1):
                role_icon = "ğŸ‘¤" if r['role'] == "user" else "ğŸ¤–"
                content_preview = r['content'][:200] + "..." if len(r['content']) > 200 else r['content']
                
                msg += f"### {i}. {role_icon} {r['role'].title()}\n\n"
                msg += f"**From:** {r['conversation_title']}\n\n"
                msg += f"> {content_preview}\n\n"
                msg += f"_ğŸ“… {r['created_at']}_\n\n"
                msg += "---\n\n"
            
            await cl.Message(content=msg).send()
        else:
            await cl.Message(content=f"âŒ KhÃ´ng tÃ¬m tháº¥y káº¿t quáº£ cho `{query}`\n\n_Thá»­ tá»« khÃ³a khÃ¡c hoáº·c kiá»ƒm tra chÃ­nh táº£_").send()
        return
    
    if user_input.lower() == "/export":
        """Export current conversation"""
        history_mgr = get_history_manager()
        conv_id = cl.user_session.get("conversation_id")
        
        try:
            md_content = history_mgr.export_conversation_markdown(conv_id)
            
            # Save to file
            export_dir = Path("data/exports")
            export_dir.mkdir(parents=True, exist_ok=True)
            export_path = export_dir / f"conversation_{conv_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            export_path.write_text(md_content, encoding="utf-8")
            
            await cl.Message(
                content=f"âœ… ÄÃ£ export conversation!\n\nğŸ“ File: `{export_path}`\n\n_Download file tá»« folder `data/exports`_"
            ).send()
        except Exception as e:
            await cl.Message(content=f"âŒ Lá»—i khi export: {str(e)}").send()
        return
    
    if user_input.lower() == "/stats":
        """Show usage statistics"""
        history_mgr = get_history_manager()
        stats = history_mgr.get_stats()
        
        msg = f"""## ğŸ“Š Thá»‘ng kÃª Há»‡ thá»‘ng

**Tá»•ng Conversations:** {stats['total_conversations']}
**Tá»•ng Messages:** {stats['total_messages']}
**Trung bÃ¬nh Messages/Conversation:** {stats['avg_messages_per_conv']:.1f}

**PhÃ¢n bá»‘ Messages:**
"""
        for role, count in stats.get('message_distribution', {}).items():
            msg += f"- {role.title()}: {count}\n"
        
        await cl.Message(content=msg).send()
        return
    
    # Ignore empty messages
    if not user_input:
        return
    
    # Check if there are any documents
    files = list_files()
    if not files:
        await cl.Message(
            content="âš ï¸ ChÆ°a cÃ³ tÃ i liá»‡u nÃ o Ä‘Æ°á»£c náº¡p. Vui lÃ²ng Ä‘Ã­nh kÃ¨m file PDF/TXT Ä‘á»ƒ báº¯t Ä‘áº§u."
        ).send()
        return
    
    # Create response message
    msg = cl.Message(content="")
    await msg.send()
    
    try:
        # Get answer
        result = await asyncio.to_thread(sync_process_query, user_input)
        
        answer = result["answer"]
        context = result["context"]
        metadatas = result["metadatas"]
        citations_md = result.get("citations_md", "")
        retrieval_info = result.get("retrieval_info", {})
        
        # Build full response with citations
        full_answer = answer
        if citations_md:
            full_answer += citations_md
        
        # Add retrieval info badge
        if retrieval_info:
            method = retrieval_info.get("method", "dense")
            reranked = retrieval_info.get("reranked", False)
            badges = []
            if method == "hybrid":
                badges.append("ğŸ” Hybrid")
            if reranked:
                badges.append("ğŸ“Š Reranked")
            if badges:
                full_answer += f"\n\n_{'  '.join(badges)}_"
        
        # Update message with answer
        msg.content = full_answer
        
        # Show context as expandable elements
        if context:
            elements = []
            for i, (doc, meta) in enumerate(zip(context, metadatas)):
                source = meta.get("source", "Unknown")
                chunk_idx = meta.get("chunk_index", i)
                
                # Create text element for context
                elements.append(
                    cl.Text(
                        name=f"ğŸ“– Nguá»“n {i+1}: {source} (chunk {chunk_idx})",
                        content=doc,
                        display="side"
                    )
                )
            
            msg.elements = elements
        
        await msg.update()
        
        # Save to database (persistent storage)
        history_mgr = get_history_manager()
        conv_id = cl.user_session.get("conversation_id")
        
        # Save user message
        history_mgr.add_message(
            conversation_id=conv_id,
            role="user",
            content=user_input
        )
        
        # Save assistant message with full context
        history_mgr.add_message(
            conversation_id=conv_id,
            role="assistant",
            content=answer,
            context=context,
            metadatas=metadatas,
            citations=citations_md
        )
        
        # Update in-memory history (for current session)
        history = cl.user_session.get("history", [])
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": answer})
        cl.user_session.set("history", history)
        
    except Exception as e:
        msg.content = f"âŒ ÄÃ£ xáº£y ra lá»—i: {str(e)}"
        await msg.update()
